{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"video_tracked1.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display, Video\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Define MTCNN module\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# Open video file\n",
    "video_path = '19.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "dim = (frame_width, frame_height)\n",
    "\n",
    "frames_tracked = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB and PIL image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame_pil)\n",
    "    \n",
    "    # Draw faces\n",
    "    frame_draw = frame_pil.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "    \n",
    "    # Convert back to OpenCV format and resize\n",
    "    frame_tracked = np.array(frame_draw)\n",
    "    frame_tracked = cv2.cvtColor(frame_tracked, cv2.COLOR_RGB2BGR)\n",
    "    frame_tracked = cv2.resize(frame_tracked, (640, 360))\n",
    "    \n",
    "    frames_tracked.append(frame_tracked)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save tracked video\n",
    "out_video_path = 'video_tracked1.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec\n",
    "video_tracked = cv2.VideoWriter(out_video_path, fourcc, fps, (640, 360))\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()\n",
    "\n",
    "# Display the tracked video\n",
    "display(Video(out_video_path, width=640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the face recognition model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Define the directory containing images of known faces\n",
    "known_faces_dir = r'D:\\Friend\\Super AI\\V89\\My Project\\facenet-pytorch-master\\data\\test_images_aligned'  # Change this to your directory\n",
    "known_face_names = []\n",
    "known_face_embeddings = []\n",
    "\n",
    "# Loop through each person in the directory\n",
    "for person_name in os.listdir(known_faces_dir):\n",
    "    person_dir = os.path.join(known_faces_dir, person_name)\n",
    "    \n",
    "    if os.path.isdir(person_dir):  # Ensure it's a directory\n",
    "        known_face_names.append(person_name)  # Add person's name\n",
    "\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            image = Image.open(image_path).convert('RGB')  # Load and convert image\n",
    "            image = image.resize((160, 160))  # Resize to (160, 160) for FaceNet\n",
    "            image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Compute embedding\n",
    "            with torch.no_grad():\n",
    "                embedding = model(image_tensor).cpu()\n",
    "                known_face_embeddings.append(embedding)\n",
    "\n",
    "# Convert list of embeddings to a tensor for easier comparison later\n",
    "known_face_embeddings = torch.stack(known_face_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n",
      "Embedding Akada Iamong_embedding shape: torch.Size([1, 512])\n",
      "Embedding Apiwat Rattanaphan_embedding shape: torch.Size([1, 512])\n",
      "Embedding Apiwit Buachan_embedding shape: torch.Size([1, 512])\n",
      "Embedding Ekapoj Suthiwong_embedding shape: torch.Size([1, 512])\n",
      "Embedding Nattawat Thakhamho_embedding shape: torch.Size([1, 512])\n",
      "Embedding Panupong Sitthiprom_embedding shape: torch.Size([1, 512])\n",
      "Embedding Peerakarn Phraphinyokul_embedding shape: torch.Size([1, 512])\n",
      "Embedding Phacharadanai Rossoda_embedding shape: torch.Size([1, 512])\n",
      "Embedding Phurin Rueannimit_embedding shape: torch.Size([1, 512])\n",
      "Embedding Sittikorn Thongdeenok_embedding shape: torch.Size([1, 512])\n",
      "Embedding Thanaphum Hengarun_embedding shape: torch.Size([1, 512])\n",
      "Embedding Thanapon Phetprapai_embedding shape: torch.Size([1, 512])\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3256340026855469, 1.269302487373352, 1.1831815242767334, 1.242719292640686, 1.3734323978424072, 1.2183698415756226, 1.30418062210083, 1.3058736324310303, 1.1800448894500732, 1.317065715789795, 1.3159394264221191, 1.2294968366622925]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3099747896194458, 1.2670990228652954, 1.1674503087997437, 1.2339317798614502, 1.370186448097229, 1.220842719078064, 1.295220971107483, 1.2863839864730835, 1.166103720664978, 1.314750075340271, 1.3116960525512695, 1.2240333557128906]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3126369714736938, 1.263167142868042, 1.169351577758789, 1.2349501848220825, 1.369307518005371, 1.2180237770080566, 1.2948312759399414, 1.290132761001587, 1.1713314056396484, 1.3074582815170288, 1.3081426620483398, 1.2187979221343994]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3066188097000122, 1.2534812688827515, 1.1635093688964844, 1.233036756515503, 1.3699309825897217, 1.2109289169311523, 1.284257173538208, 1.2849613428115845, 1.1650968790054321, 1.2968741655349731, 1.3021631240844727, 1.2134157419204712]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3183557987213135, 1.2509183883666992, 1.1753334999084473, 1.2422351837158203, 1.3754647970199585, 1.2174872159957886, 1.2853021621704102, 1.2898414134979248, 1.1777781248092651, 1.3015124797821045, 1.3059006929397583, 1.220824122428894]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3252817392349243, 1.2567988634109497, 1.1891266107559204, 1.246773600578308, 1.3810901641845703, 1.223855972290039, 1.2941780090332031, 1.2944459915161133, 1.179917812347412, 1.3139616250991821, 1.3078240156173706, 1.226131796836853]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.336901068687439, 1.26041579246521, 1.202713966369629, 1.2586215734481812, 1.390626072883606, 1.2266607284545898, 1.2983741760253906, 1.3123072385787964, 1.1951841115951538, 1.3190972805023193, 1.3203589916229248, 1.2399053573608398]\n",
      "Identified: Unknown\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 64\u001b[0m\n\u001b[0;32m     61\u001b[0m frame_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame_rgb)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m boxes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmtcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_pil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Draw faces\u001b[39;00m\n\u001b[0;32m     67\u001b[0m frame_draw \u001b[38;5;241m=\u001b[39m frame_pil\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:73\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m     71\u001b[0m im_data \u001b[38;5;241m=\u001b[39m imresample(imgs, (\u001b[38;5;28mint\u001b[39m(h \u001b[38;5;241m*\u001b[39m scale \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;28mint\u001b[39m(w \u001b[38;5;241m*\u001b[39m scale \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m     72\u001b[0m im_data \u001b[38;5;241m=\u001b[39m (im_data \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m127.5\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0078125\u001b[39m\n\u001b[1;32m---> 73\u001b[0m reg, probs \u001b[38;5;241m=\u001b[39m \u001b[43mpnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m boxes_scale, image_inds_scale \u001b[38;5;241m=\u001b[39m generateBoundingBox(reg, probs[:, \u001b[38;5;241m1\u001b[39m], scale, threshold[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     76\u001b[0m boxes\u001b[38;5;241m.\u001b[39mappend(boxes_scale)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:43\u001b[0m, in \u001b[0;36mPNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)\n\u001b[0;32m     42\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprelu2(x)\n\u001b[1;32m---> 43\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprelu3(x)\n\u001b[0;32m     45\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv4_1(x)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display, Video\n",
    "import os\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Define MTCNN module\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# Load the face recognition model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Define path to your embeddings folder\n",
    "embeddings_folder = r'D:\\Friend\\Super AI\\V89\\My Project\\facenet-pytorch-master\\examples\\filept'  # Change this to your embeddings folder path\n",
    "known_face_names = []\n",
    "known_face_embeddings = []\n",
    "\n",
    "# Load known face embeddings from the folder\n",
    "for filename in os.listdir(embeddings_folder):\n",
    "    if filename.endswith('.pt'):\n",
    "        name = filename[:-3]  # Remove the .pt extension to get the name\n",
    "        embedding_path = os.path.join(embeddings_folder, filename)\n",
    "        embedding = torch.load(embedding_path).to(device)  # Load and move to device\n",
    "        known_face_names.append(name)\n",
    "        known_face_embeddings.append(embedding)\n",
    "\n",
    "# Debug print to check the size of known embeddings\n",
    "for idx, embedding in enumerate(known_face_embeddings):\n",
    "    print(f\"Embedding {known_face_names[idx]} shape: {embedding.shape}\")\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_face(embedding, known_face_embeddings, known_face_names, threshold=0.8):\n",
    "    distances = [torch.norm(embedding - known_face_embedding).item() for known_face_embedding in known_face_embeddings]\n",
    "    min_distance_index = np.argmin(distances)\n",
    "    print(f\"Distances: {distances}\")  # Debug print\n",
    "    if distances[min_distance_index] < threshold:\n",
    "        return known_face_names[min_distance_index]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Open video file\n",
    "video_path = '15.mp4'  # Path to your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frames_tracked = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB and PIL image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame_pil)\n",
    "    \n",
    "    # Draw faces\n",
    "    frame_draw = frame_pil.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)\n",
    "            # Get the face region for recognition\n",
    "            face_region = frame_rgb[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            face_region = cv2.resize(face_region, (160, 160))  # Resize for FaceNet\n",
    "            face_tensor = torch.tensor(face_region).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Get embedding and recognize\n",
    "            with torch.no_grad():\n",
    "                embedding = model(face_tensor).cpu()\n",
    "                print(f\"Current embedding shape: {embedding.shape}\")  # Debug print\n",
    "                identity = recognize_face(embedding, known_face_embeddings, known_face_names)\n",
    "\n",
    "            print(f\"Identified: {identity}\")  # Debug print\n",
    "            \n",
    "            # Draw name on the frame (create a background for better visibility)\n",
    "            text = identity\n",
    "            text_bbox = draw.textbbox((box[0], box[1] - 10), text)  # Get the bounding box of the text\n",
    "            draw.rectangle([text_bbox[0], text_bbox[1], text_bbox[2], text_bbox[3]], fill=(255, 0, 0))  # Draw background rectangle\n",
    "            draw.text((box[0], box[1] - 10), text, fill=(255, 255, 255))  # Draw name above the box\n",
    "\n",
    "    # Convert back to OpenCV format and resize\n",
    "    frame_tracked = np.array(frame_draw)\n",
    "    frame_tracked = cv2.cvtColor(frame_tracked, cv2.COLOR_RGB2BGR)\n",
    "    frame_tracked = cv2.resize(frame_tracked, (640, 360))\n",
    "    \n",
    "    frames_tracked.append(frame_tracked)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save tracked video\n",
    "out_video_path = 'video_tracked.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec\n",
    "video_tracked = cv2.VideoWriter(out_video_path, fourcc, fps, (640, 360))\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()\n",
    "\n",
    "# Display the tracked video\n",
    "display(Video(out_video_path, width=640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_face_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0430, -0.0596, -0.0278,  ..., -0.0368, -0.0273, -0.0660],\n",
       "        [ 0.0479, -0.0599, -0.0065,  ..., -0.0566, -0.0555, -0.0496],\n",
       "        [ 0.0101, -0.0197, -0.0148,  ..., -0.0644,  0.0151, -0.0436],\n",
       "        ...,\n",
       "        [-0.0396, -0.0635,  0.0135,  ..., -0.0466, -0.0233, -0.0844],\n",
       "        [ 0.0121, -0.0422, -0.0076,  ...,  0.0134, -0.0427, -0.0213],\n",
       "        [ 0.0312, -0.0216, -0.0049,  ..., -0.0577, -0.0461, -0.0355]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_face_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] The directory name is invalid: 'D:\\\\Friend\\\\Super AI\\\\V89\\\\My Project\\\\facenet-pytorch-master\\\\examples\\\\embeddings.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m known_face_embeddings \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Load known face embeddings from the folder\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     27\u001b[0m         name \u001b[38;5;241m=\u001b[39m filename[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m3\u001b[39m]  \u001b[38;5;66;03m# Remove the .pt extension to get the name\u001b[39;00m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] The directory name is invalid: 'D:\\\\Friend\\\\Super AI\\\\V89\\\\My Project\\\\facenet-pytorch-master\\\\examples\\\\embeddings.pt'"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display, Video\n",
    "import os\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Define MTCNN module\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# Load the face recognition model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Define path to your embeddings folder\n",
    "embeddings_folder = r'D:\\Friend\\Super AI\\V89\\My Project\\facenet-pytorch-master\\examples\\embeddings.pt'  # Change this to your embeddings folder path\n",
    "known_face_names = []\n",
    "known_face_embeddings = []\n",
    "\n",
    "# Load known face embeddings from the folder\n",
    "for filename in os.listdir(embeddings_folder):\n",
    "    if filename.endswith('.pt'):\n",
    "        name = filename[:-3]  # Remove the .pt extension to get the name\n",
    "        embedding_path = os.path.join(embeddings_folder, filename)\n",
    "        embedding = torch.load(embedding_path).to(device)  # Load and move to device\n",
    "        known_face_names.append(name)\n",
    "        known_face_embeddings.append(embedding)\n",
    "\n",
    "# Convert list of embeddings to a tensor for easier comparison later\n",
    "known_face_embeddings = torch.stack(known_face_embeddings)\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_face(embedding, known_face_embeddings, known_face_names, threshold=0.8):\n",
    "    distances = [torch.norm(embedding - known_face_embedding).item() for known_face_embedding in known_face_embeddings]\n",
    "    min_distance_index = np.argmin(distances)\n",
    "    if distances[min_distance_index] < threshold:\n",
    "        return known_face_names[min_distance_index]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Open video file\n",
    "video_path = '15.mp4'  # Path to your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frames_tracked = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB and PIL image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame_pil)\n",
    "    \n",
    "    # Draw faces and names\n",
    "    frame_draw = frame_pil.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)  # Draw rectangle around face\n",
    "            # Get the face region for recognition\n",
    "            face_region = frame_rgb[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            face_region = cv2.resize(face_region, (160, 160))  # Resize for FaceNet\n",
    "            face_tensor = torch.tensor(face_region).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Get embedding and recognize\n",
    "            with torch.no_grad():\n",
    "                embedding = model(face_tensor).cpu()\n",
    "                identity = recognize_face(embedding, known_face_embeddings, known_face_names)\n",
    "\n",
    "            # Draw name on the frame\n",
    "            text_position = (int(box[0]), int(box[1]) - 10)  # Position to draw the name above the rectangle\n",
    "            draw.text(text_position, identity, fill=(255, 0, 0))  # Draw name\n",
    "\n",
    "    # Convert back to OpenCV format and resize\n",
    "    frame_tracked = np.array(frame_draw)\n",
    "    frame_tracked = cv2.cvtColor(frame_tracked, cv2.COLOR_RGB2BGR)\n",
    "    frame_tracked = cv2.resize(frame_tracked, (640, 360))\n",
    "    \n",
    "    frames_tracked.append(frame_tracked)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save tracked video\n",
    "out_video_path = 'video_tracked2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec\n",
    "video_tracked = cv2.VideoWriter(out_video_path, fourcc, fps, (640, 360))\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()\n",
    "\n",
    "# Display the tracked video\n",
    "display(Video(out_video_path, width=640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cpu\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video src=\"video_tracked2.mp4\" controls  width=\"640\" >\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display, Video\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print('Running on device: {}'.format(device))\n",
    "\n",
    "# Define MTCNN module\n",
    "mtcnn = MTCNN(keep_all=True, device=device)\n",
    "\n",
    "# Load the face recognition model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Load embeddings from the file 'embeddings.pt'\n",
    "embeddings_path = r'D:\\Friend\\Super AI\\V89\\My Project\\facenet-pytorch-master\\examples\\embeddings.pt'  # Change this to your file path\n",
    "known_face_embeddings = torch.load(embeddings_path).to(device)  # Assume this is a tensor\n",
    "\n",
    "# List of known face names (in the same order as the embeddings in the tensor)\n",
    "known_face_names = []  # Update this with the actual names corresponding to embeddings\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_face(embedding, known_face_embeddings, known_face_names, threshold=0.8):\n",
    "    distances = [torch.norm(embedding - known_face_embedding).item() for known_face_embedding in known_face_embeddings]\n",
    "    min_distance_index = np.argmin(distances)\n",
    "    if distances[min_distance_index] < threshold:\n",
    "        return known_face_names[min_distance_index]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Open video file\n",
    "video_path = '2.mp4'  # Path to your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frames_tracked = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB and PIL image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame_pil)\n",
    "    \n",
    "    # Draw faces and names\n",
    "    frame_draw = frame_pil.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)  # Draw rectangle around face\n",
    "            # Get the face region for recognition\n",
    "            face_region = frame_rgb[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            face_region = cv2.resize(face_region, (160, 160))  # Resize for FaceNet\n",
    "            face_tensor = torch.tensor(face_region).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Get embedding and recognize\n",
    "            with torch.no_grad():\n",
    "                embedding = model(face_tensor).cpu()\n",
    "                identity = recognize_face(embedding, known_face_embeddings, known_face_names)\n",
    "\n",
    "            # Draw name on the frame\n",
    "            text_position = (int(box[0]), int(box[1]) - 10)  # Position to draw the name above the rectangle\n",
    "            draw.text(text_position, identity, fill=(255, 0, 0))  # Draw name\n",
    "\n",
    "    # Convert back to OpenCV format and resize\n",
    "    frame_tracked = np.array(frame_draw)\n",
    "    frame_tracked = cv2.cvtColor(frame_tracked, cv2.COLOR_RGB2BGR)\n",
    "    frame_tracked = cv2.resize(frame_tracked, (640, 360))\n",
    "    \n",
    "    frames_tracked.append(frame_tracked)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save tracked video\n",
    "out_video_path = 'video_tracked2.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec\n",
    "video_tracked = cv2.VideoWriter(out_video_path, fourcc, fps, (640, 360))\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()\n",
    "\n",
    "# Display the tracked video\n",
    "display(Video(out_video_path, width=640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 74\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Get the face region for recognition\u001b[39;00m\n\u001b[0;32m     73\u001b[0m face_region \u001b[38;5;241m=\u001b[39m frame_rgb[\u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m1\u001b[39m]):\u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m3\u001b[39m]), \u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m0\u001b[39m]):\u001b[38;5;28mint\u001b[39m(box[\u001b[38;5;241m2\u001b[39m])]\n\u001b[1;32m---> 74\u001b[0m face_region \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_region\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m160\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Resize for FaceNet\u001b[39;00m\n\u001b[0;32m     75\u001b[0m face_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(face_region)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Get embedding and recognize\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import InceptionResnetV1\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the face recognition model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "\n",
    "# Define the directory containing images of known faces\n",
    "known_faces_dir = r'D:\\Friend\\Super AI\\V89\\My Project\\facenet-pytorch-master\\data\\test_images_aligned'  # Change this to your directory\n",
    "known_face_names = []\n",
    "known_face_embeddings = []\n",
    "\n",
    "# Loop through each person in the directory\n",
    "for person_name in os.listdir(known_faces_dir):\n",
    "    person_dir = os.path.join(known_faces_dir, person_name)\n",
    "    \n",
    "    if os.path.isdir(person_dir):  # Ensure it's a directory\n",
    "        known_face_names.append(person_name)  # Add person's name\n",
    "\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            image = Image.open(image_path).convert('RGB')  # Load and convert image\n",
    "            image = image.resize((160, 160))  # Resize to (160, 160) for FaceNet\n",
    "            image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Compute embedding\n",
    "            with torch.no_grad():\n",
    "                embedding = model(image_tensor).cpu()\n",
    "                known_face_embeddings.append(embedding)\n",
    "\n",
    "# Convert list of embeddings to a tensor for easier comparison later\n",
    "known_face_embeddings = torch.stack(known_face_embeddings)\n",
    "# Function to recognize faces\n",
    "def recognize_face(embedding, known_face_embeddings, known_face_names, threshold=0.8):\n",
    "    distances = [torch.norm(embedding - known_face_embedding).item() for known_face_embedding in known_face_embeddings]\n",
    "    min_distance_index = np.argmin(distances)\n",
    "    if distances[min_distance_index] < threshold:\n",
    "        return known_face_names[min_distance_index]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Open video file\n",
    "video_path = '3.mp4'  # Path to your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frames_tracked = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB and PIL image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame_pil)\n",
    "    \n",
    "    # Draw faces and names\n",
    "    frame_draw = frame_pil.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "            draw.rectangle(box.tolist(), outline=(255, 0, 0), width=6)  # Draw rectangle around face\n",
    "            # Get the face region for recognition\n",
    "            face_region = frame_rgb[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            face_region = cv2.resize(face_region, (160, 160))  # Resize for FaceNet\n",
    "            face_tensor = torch.tensor(face_region).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Get embedding and recognize\n",
    "            with torch.no_grad():\n",
    "                embedding = model(face_tensor).cpu()\n",
    "                identity = recognize_face(embedding, known_face_embeddings, known_face_names)\n",
    "\n",
    "            # Draw name on the frame\n",
    "            # Draw name on the frame\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            text_position = (int(box[0]), int(box[1]) - 10)  # Position to draw the name above the rectangle\n",
    "            cv2.putText(frame_tracked, identity, text_position, font, 0.8, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "    # Convert back to OpenCV format and resize\n",
    "    frame_tracked = np.array(frame_draw)\n",
    "    frame_tracked = cv2.cvtColor(frame_tracked, cv2.COLOR_RGB2BGR)\n",
    "    frame_tracked = cv2.resize(frame_tracked, (640, 360))\n",
    "    \n",
    "    frames_tracked.append(frame_tracked)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save tracked video\n",
    "out_video_path = 'video_tracked3.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec\n",
    "video_tracked = cv2.VideoWriter(out_video_path, fourcc, fps, (640, 360))\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()\n",
    "\n",
    "# Display the tracked video\n",
    "display(Video(out_video_path, width=640))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.326780915260315, 1.333076000213623, 1.3614587783813477, 1.3470032215118408, 1.363025188446045, 1.300369143486023, 1.311286211013794, 1.4173860549926758, 1.3790677785873413, 1.4023091793060303, 1.3307775259017944, 1.3263463973999023, 1.333714246749878, 1.3229399919509888, 1.3356091976165771, 1.2852085828781128, 1.3030695915222168, 1.355961561203003, 1.3216259479522705, 1.3878012895584106, 1.3270856142044067]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3377186059951782, 1.3363120555877686, 1.365031361579895, 1.3551883697509766, 1.3691855669021606, 1.3098164796829224, 1.322850227355957, 1.4259612560272217, 1.388258695602417, 1.4068241119384766, 1.3460469245910645, 1.3279014825820923, 1.3401379585266113, 1.3303840160369873, 1.3463068008422852, 1.2874234914779663, 1.30964195728302, 1.3617446422576904, 1.3290356397628784, 1.39802885055542, 1.3313701152801514]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.2721810340881348, 1.2779697179794312, 1.3116990327835083, 1.2961634397506714, 1.3054254055023193, 1.2379801273345947, 1.2492408752441406, 1.3643078804016113, 1.331301212310791, 1.3508541584014893, 1.2821500301361084, 1.2752912044525146, 1.275748610496521, 1.25966215133667, 1.284728765487671, 1.247067928314209, 1.2540265321731567, 1.3252955675125122, 1.2829943895339966, 1.3330389261245728, 1.2749648094177246]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.2655607461929321, 1.2662206888198853, 1.3016462326049805, 1.2950646877288818, 1.30354642868042, 1.2359886169433594, 1.243354320526123, 1.3615461587905884, 1.3300445079803467, 1.342664361000061, 1.285491943359375, 1.264137864112854, 1.2760851383209229, 1.2620316743850708, 1.2805441617965698, 1.2366398572921753, 1.2453439235687256, 1.3237320184707642, 1.2870008945465088, 1.3318909406661987, 1.2691411972045898]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.2707359790802002, 1.2766679525375366, 1.3103448152542114, 1.2958855628967285, 1.3058876991271973, 1.239053726196289, 1.2505627870559692, 1.3723286390304565, 1.338011622428894, 1.349802017211914, 1.2885420322418213, 1.2730056047439575, 1.2760242223739624, 1.2624809741973877, 1.2863337993621826, 1.2393572330474854, 1.25148606300354, 1.3247109651565552, 1.2844197750091553, 1.337776780128479, 1.2735408544540405]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3279560804367065, 1.3271228075027466, 1.3564773797988892, 1.3413136005401611, 1.3591378927230835, 1.2979074716567993, 1.3095290660858154, 1.4169557094573975, 1.3810981512069702, 1.3909063339233398, 1.3415896892547607, 1.318089485168457, 1.3246115446090698, 1.3101295232772827, 1.3353077173233032, 1.287718653678894, 1.2981096506118774, 1.3655849695205688, 1.3284064531326294, 1.3921680450439453, 1.31974458694458]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.2750853300094604, 1.277418613433838, 1.3133776187896729, 1.3036738634109497, 1.3138514757156372, 1.2453457117080688, 1.254755973815918, 1.3732575178146362, 1.3395538330078125, 1.3546204566955566, 1.2954713106155396, 1.2775866985321045, 1.2854186296463013, 1.2738465070724487, 1.2895948886871338, 1.2477229833602905, 1.2579230070114136, 1.3260351419448853, 1.2878838777542114, 1.3447238206863403, 1.278271198272705]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.2862589359283447, 1.2907623052597046, 1.3238768577575684, 1.310579776763916, 1.3226438760757446, 1.2550532817840576, 1.2682137489318848, 1.3850575685501099, 1.3491920232772827, 1.3673652410507202, 1.3005130290985107, 1.2906321287155151, 1.2960842847824097, 1.2812514305114746, 1.2987819910049438, 1.2550891637802124, 1.2676804065704346, 1.3319231271743774, 1.2898586988449097, 1.3541446924209595, 1.283677339553833]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3135017156600952, 1.3107951879501343, 1.3433144092559814, 1.3330496549606323, 1.3417946100234985, 1.2807432413101196, 1.291243076324463, 1.39510178565979, 1.3602582216262817, 1.37761652469635, 1.323341727256775, 1.3006266355514526, 1.311545491218567, 1.2972851991653442, 1.323687195777893, 1.2742317914962769, 1.28367018699646, 1.3506901264190674, 1.3140634298324585, 1.3749382495880127, 1.3072729110717773]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3157800436019897, 1.3111110925674438, 1.34347403049469, 1.3335455656051636, 1.3433014154434204, 1.2830159664154053, 1.2960433959960938, 1.3945953845977783, 1.3597402572631836, 1.3808711767196655, 1.3257696628570557, 1.3026912212371826, 1.3105568885803223, 1.2969586849212646, 1.3239389657974243, 1.2762451171875, 1.2853665351867676, 1.3451602458953857, 1.3115500211715698, 1.374537467956543, 1.3071269989013672]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3140227794647217, 1.3233174085617065, 1.3514630794525146, 1.3363326787948608, 1.3431925773620605, 1.2831858396530151, 1.2953429222106934, 1.4119627475738525, 1.3765084743499756, 1.3898545503616333, 1.3237922191619873, 1.3153902292251587, 1.3163548707962036, 1.2939695119857788, 1.3310675621032715, 1.2818336486816406, 1.2999612092971802, 1.3657318353652954, 1.3231799602508545, 1.3809974193572998, 1.3128591775894165]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3125832080841064, 1.3229032754898071, 1.3510669469833374, 1.3348751068115234, 1.3432183265686035, 1.2818646430969238, 1.2939958572387695, 1.4125890731811523, 1.377156376838684, 1.3897114992141724, 1.3229727745056152, 1.3149152994155884, 1.3162949085235596, 1.2947728633880615, 1.329691767692566, 1.280439019203186, 1.2986235618591309, 1.363123893737793, 1.3211290836334229, 1.3809659481048584, 1.3128139972686768]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3276875019073486, 1.3268898725509644, 1.3582466840744019, 1.344136118888855, 1.3576099872589111, 1.29556143283844, 1.3116726875305176, 1.4085532426834106, 1.3707408905029297, 1.4000582695007324, 1.331783413887024, 1.3195483684539795, 1.3207858800888062, 1.3082844018936157, 1.3340768814086914, 1.2847673892974854, 1.2948296070098877, 1.3499959707260132, 1.3157858848571777, 1.3862080574035645, 1.3201310634613037]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3130875825881958, 1.3189888000488281, 1.3488155603408813, 1.3347032070159912, 1.341835379600525, 1.2816145420074463, 1.293196678161621, 1.4078477621078491, 1.3732600212097168, 1.3866595029830933, 1.3245197534561157, 1.3100413084030151, 1.3108577728271484, 1.288926362991333, 1.3268502950668335, 1.2792147397994995, 1.2941601276397705, 1.3631775379180908, 1.3239989280700684, 1.378768801689148, 1.3097352981567383]\n",
      "Identified: Unknown\n",
      "Current embedding shape: torch.Size([1, 512])\n",
      "Distances: [1.3097530603408813, 1.3107199668884277, 1.343105435371399, 1.332444190979004, 1.3383398056030273, 1.27734375, 1.2908307313919067, 1.4052138328552246, 1.3701236248016357, 1.3764547109603882, 1.3248597383499146, 1.3008549213409424, 1.3076989650726318, 1.2866932153701782, 1.3252050876617432, 1.272377371788025, 1.288344383239746, 1.3608394861221313, 1.319632649421692, 1.3800160884857178, 1.303955078125]\n",
      "Identified: Unknown\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 68\u001b[0m\n\u001b[0;32m     65\u001b[0m frame_pil \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(frame_rgb)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Detect faces\u001b[39;00m\n\u001b[1;32m---> 68\u001b[0m boxes, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmtcnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_pil\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Draw faces and names\u001b[39;00m\n\u001b[0;32m     71\u001b[0m frame_draw \u001b[38;5;241m=\u001b[39m frame_pil\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\mtcnn.py:313\u001b[0m, in \u001b[0;36mMTCNN.detect\u001b[1;34m(self, img, landmarks)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Detect all faces in PIL image and return bounding boxes and optional facial landmarks.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m \n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03mThis method is used by the forward method and is also useful for face detection tasks\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    309\u001b[0m \u001b[38;5;124;03m>>> img_draw.save('annotated_faces.png')\u001b[39;00m\n\u001b[0;32m    310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 313\u001b[0m     batch_boxes, batch_points \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_face_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43monet\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mthresholds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    320\u001b[0m boxes, probs, points \u001b[38;5;241m=\u001b[39m [], [], []\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m box, point \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(batch_boxes, batch_points):\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:112\u001b[0m, in \u001b[0;36mdetect_face\u001b[1;34m(imgs, minsize, pnet, rnet, onet, threshold, factor, device)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ey[k] \u001b[38;5;241m>\u001b[39m (y[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m ex[k] \u001b[38;5;241m>\u001b[39m (x[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    111\u001b[0m         img_k \u001b[38;5;241m=\u001b[39m imgs[image_inds[k], :, (y[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):ey[k], (x[k] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m):ex[k]]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 112\u001b[0m         im_data\u001b[38;5;241m.\u001b[39mappend(\u001b[43mimresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    113\u001b[0m im_data \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(im_data, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    114\u001b[0m im_data \u001b[38;5;241m=\u001b[39m (im_data \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m127.5\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.0078125\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\facenet_pytorch\\models\\utils\\detect_face.py:305\u001b[0m, in \u001b[0;36mimresample\u001b[1;34m(img, sz)\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mimresample\u001b[39m(img, sz):\n\u001b[1;32m--> 305\u001b[0m     im_data \u001b[38;5;241m=\u001b[39m \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marea\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im_data\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:4017\u001b[0m, in \u001b[0;36minterpolate\u001b[1;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[0m\n\u001b[0;32m   4015\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4016\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4018\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marea\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   4019\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m output_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\HP\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1233\u001b[0m, in \u001b[0;36madaptive_avg_pool2d\u001b[1;34m(input, output_size)\u001b[0m\n\u001b[0;32m   1231\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(adaptive_avg_pool2d, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, output_size)\n\u001b[0;32m   1232\u001b[0m _output_size \u001b[38;5;241m=\u001b[39m _list_with_default(output_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 1233\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madaptive_avg_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_output_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import InceptionResnetV1, MTCNN\n",
    "import torch\n",
    "from PIL import Image, ImageDraw\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Determine if an NVIDIA GPU is available\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the face recognition model\n",
    "model = InceptionResnetV1(pretrained='vggface2').eval().to(device)\n",
    "mtcnn = MTCNN(keep_all=True, device=device)  # Initialize MTCNN for face detection\n",
    "\n",
    "# Define the directory containing images of known faces\n",
    "known_faces_dir = r'D:\\Friend\\Super AI\\V89\\My Project\\facenet-pytorch-master\\data\\test_images_aligned'  # Change this to your directory\n",
    "known_face_names = []\n",
    "known_face_embeddings = []\n",
    "\n",
    "# Loop through each person in the directory\n",
    "for person_name in os.listdir(known_faces_dir):\n",
    "    person_dir = os.path.join(known_faces_dir, person_name)\n",
    "    \n",
    "    if os.path.isdir(person_dir):  # Ensure it's a directory\n",
    "        known_face_names.append(person_name)  # Add person's name\n",
    "\n",
    "        for image_name in os.listdir(person_dir):\n",
    "            image_path = os.path.join(person_dir, image_name)\n",
    "            image = Image.open(image_path).convert('RGB')  # Load and convert image\n",
    "            image = image.resize((160, 160))  # Resize to (160, 160) for FaceNet\n",
    "            image_tensor = torch.tensor(np.array(image)).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "            \n",
    "            # Compute embedding\n",
    "            with torch.no_grad():\n",
    "                embedding = model(image_tensor).cpu()\n",
    "                known_face_embeddings.append(embedding)\n",
    "\n",
    "# Convert list of embeddings to a tensor for easier comparison later\n",
    "known_face_embeddings = torch.stack(known_face_embeddings)\n",
    "\n",
    "# Function to recognize faces\n",
    "def recognize_face(embedding, known_face_embeddings, known_face_names, threshold=1.0):\n",
    "    distances = [torch.norm(embedding - known_face_embedding).item() for known_face_embedding in known_face_embeddings]\n",
    "    min_distance_index = np.argmin(distances)\n",
    "    print(f\"Distances: {distances}\") \n",
    "    if distances[min_distance_index] < threshold:\n",
    "        return known_face_names[min_distance_index]\n",
    "    return \"Unknown\"\n",
    "\n",
    "# Open video file\n",
    "video_path = '2.mp4'  # Path to your video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "frames_tracked = []\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Convert frame to RGB and PIL image\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    frame_pil = Image.fromarray(frame_rgb)\n",
    "    \n",
    "    # Detect faces\n",
    "    boxes, _ = mtcnn.detect(frame_pil)\n",
    "    \n",
    "    # Draw faces and names\n",
    "    frame_draw = frame_pil.copy()\n",
    "    draw = ImageDraw.Draw(frame_draw)\n",
    "    if boxes is not None:\n",
    "        for box in boxes:\n",
    "        # Convert box coordinates to integer\n",
    "            box = box.astype(int)\n",
    "        # Draw rectangle around face using OpenCV\n",
    "            cv2.rectangle(frame, (box[0], box[1]), (box[2], box[3]), (255, 0, 0), 2)\n",
    "        \n",
    "        # Get the face region for recognition\n",
    "            face_region = frame_rgb[int(box[1]):int(box[3]), int(box[0]):int(box[2])]\n",
    "            face_region = cv2.resize(face_region, (160, 160))  # Resize for FaceNet\n",
    "            face_tensor = torch.tensor(face_region).permute(2, 0, 1).float().unsqueeze(0).to(device) / 255.0\n",
    "        \n",
    "        # Get embedding and recognize\n",
    "            with torch.no_grad():\n",
    "                embedding = model(face_tensor).cpu()\n",
    "                print(f\"Current embedding shape: {embedding.shape}\")\n",
    "                identity = recognize_face(embedding, known_face_embeddings, known_face_names)\n",
    "        \n",
    "            print(f\"Identified: {identity}\")\n",
    "        \n",
    "        # Draw name on the frame\n",
    "            text_position = (int(box[0]), int(box[1]) - 10)  # Position to draw the name above the rectangle\n",
    "            cv2.putText(frame, identity, text_position, cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 0, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    # Resize frame for output\n",
    "    frame_tracked = cv2.resize(frame, (640, 360))\n",
    "    frames_tracked.append(frame_tracked)\n",
    "\n",
    "cap.release()\n",
    "\n",
    "# Save tracked video\n",
    "out_video_path = 'video_tracked5.mp4'\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Use 'mp4v' codec\n",
    "video_tracked = cv2.VideoWriter(out_video_path, fourcc, fps, (640, 360))\n",
    "for frame in frames_tracked:\n",
    "    video_tracked.write(frame)\n",
    "video_tracked.release()\n",
    "\n",
    "# Display the tracked video\n",
    "display(Video(out_video_path, width=640))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Akada Iamong',\n",
       " 'Akaradej Sukchan',\n",
       " 'Apiwat Rattanaphan',\n",
       " 'Apiwit Buachan',\n",
       " 'Ekapoj Suthiwong',\n",
       " 'Karanyapas Srikham',\n",
       " 'Nattawat Thakhamho',\n",
       " 'Noppawit Sabai',\n",
       " 'Panupong Sitthiprom',\n",
       " 'Peerakarn Phraphinyokul',\n",
       " 'Phacharadanai Rossoda',\n",
       " 'Phuri Khamfei',\n",
       " 'Phurin Rueannimit',\n",
       " 'Sittikorn Thongdeenok',\n",
       " 'Thanaphum Hengarun',\n",
       " 'Thanapon Phetprapai',\n",
       " 'Thanwa Suktham']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_face_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0183, -0.0476, -0.0655,  ..., -0.0362,  0.0060,  0.0063]],\n",
       "\n",
       "        [[ 0.0222, -0.0672, -0.0749,  ..., -0.0052,  0.0130, -0.0061]],\n",
       "\n",
       "        [[ 0.0288, -0.0677, -0.0765,  ..., -0.0008,  0.0098,  0.0027]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.0040, -0.0670, -0.0284,  ..., -0.0302,  0.0043, -0.0202]],\n",
       "\n",
       "        [[-0.0143, -0.0222, -0.0774,  ..., -0.0232,  0.0018, -0.0024]],\n",
       "\n",
       "        [[ 0.0309, -0.0610, -0.1038,  ..., -0.0382,  0.0125,  0.0054]]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "known_face_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
